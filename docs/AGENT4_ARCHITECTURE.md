# ğŸ—ï¸ Agent 4 Architecture Explained

## ğŸ“‹ Overview

You have **3 files** for Agent 4, but they are **NOT separate scripts** to run. They work together as a **modular architecture** with two operating modes.

---

## ğŸ¯ The Three Files

### 1ï¸âƒ£ **agent4_llm_explainer.py** (ORIGINAL - 407 lines)
**Status:** âœ… Already exists, fully working

**Purpose:** The **original** Agent 4 implementation using **Direct HTTP** to Ollama

**How it works:**
```python
# Direct HTTP POST request
response = requests.post(
    "http://localhost:11500/api/generate",
    json={"model": "qwen2.5:latest", "prompt": "..."}
)
```

**Advantages:**
- âœ… Fast (no framework overhead)
- âœ… Simple (just HTTP requests)
- âœ… Lightweight (no extra dependencies)
- âœ… Proven and working

**Class:**
```python
class LLMExplainerAgent:
    """Original implementation with Direct HTTP"""
    def generate_explanation(self, match_result) -> str:
        # Uses requests.post() directly
```

---

### 2ï¸âƒ£ **agent4_langchain_explainer.py** (NEW - 248 lines)
**Status:** ğŸ†• Just created, needs testing

**Purpose:** **Alternative** Agent 4 implementation using **LangChain framework**

**How it works:**
```python
# LangChain LCEL (Expression Language) chain
from langchain_ollama import ChatOllama
from langchain_core.prompts import PromptTemplate

self.llm = ChatOllama(model="qwen2.5:latest", base_url="...")
self.prompt = PromptTemplate(...)
self.chain = prompt | llm | StrOutputParser()

# One line to generate
explanation = self.chain.invoke(input_data)
```

**Advantages:**
- âœ… Cleaner code with prompt templates
- âœ… Streaming support (real-time responses)
- âœ… Easy to switch providers (Ollama â†’ OpenAI â†’ Claude)
- âœ… Built-in retry logic and error handling
- âœ… LangSmith tracing for debugging

**Class:**
```python
class LangChainExplainerAgent:
    """Advanced implementation with LangChain"""
    def generate_explanation(self, match_result) -> str:
        # Uses LangChain LCEL chain
```

---

### 3ï¸âƒ£ **agent4_factory.py** (NEW - 60 lines)
**Status:** ğŸ†• Just created, **CRITICAL** for switching

**Purpose:** **Factory Pattern** - Decides which Agent 4 to use

**How it works:**
```python
def get_explainer_agent(use_langchain=None, config=None):
    """
    Smart selection logic:
    1. If use_langchain=True â†’ Try LangChain (fallback to Direct HTTP)
    2. If use_langchain=False â†’ Use Direct HTTP
    3. If use_langchain=None â†’ Check config.llm.use_langchain
    """
    if use_langchain:
        try:
            from .agent4_langchain_explainer import LangChainExplainerAgent
            return LangChainExplainerAgent(config)  # Advanced mode
        except ImportError:
            # LangChain not installed, fallback
            pass
    
    # Default: Fast mode
    from .agent4_llm_explainer import LLMExplainerAgent
    return LLMExplainerAgent(config)
```

**This is the "brain" that chooses which Agent 4 to use!**

---

## ğŸ”„ How They Work Together

### Architecture Diagram:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    pipeline.py                          â”‚
â”‚  (The main orchestrator that calls agents)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ Imports and calls:
                         â”‚ from .agent4_factory import get_explainer_agent
                         â”‚ self.agent4 = get_explainer_agent(config)
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              agent4_factory.py                          â”‚
â”‚         (Decides which Agent 4 to use)                  â”‚
â”‚                                                          â”‚
â”‚  if use_langchain=True:                                 â”‚
â”‚      return LangChainExplainerAgent â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  else:                                    â”‚             â”‚
â”‚      return LLMExplainerAgent â”€â”€â”€â”€â”       â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚       â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  agent4_llm_explainer.py     â”‚      â”‚ agent4_langchain_explainer.pyâ”‚
â”‚  (Direct HTTP - Fast Mode)   â”‚      â”‚  (LangChain - Advanced Mode) â”‚
â”‚                              â”‚      â”‚                              â”‚
â”‚  Uses: requests.post()       â”‚      â”‚  Uses: ChatOllama + LCEL     â”‚
â”‚  Endpoint: localhost:11500   â”‚      â”‚  Endpoint: localhost:11500   â”‚
â”‚  Speed: âš¡ Fast (200ms)      â”‚      â”‚  Speed: ğŸ”— Moderate (300ms)  â”‚
â”‚  Dependencies: requests      â”‚      â”‚  Dependencies: langchain     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš¦ Execution Flow

### Example 1: User uploads CV with **Direct HTTP** mode (default)

```
1. User uploads CV via frontend
   â†“
2. Frontend calls API: /match?use_langchain=false
   â†“
3. api.py receives request
   â†“
4. pipeline.py runs 4 agents:
   - Agent 1: Parse CV â†’ text extraction âœ…
   - Agent 2: Extract data â†’ structured profile âœ…
   - Agent 3: Score matches â†’ calculate scores âœ…
   - Agent 4: Explain â†’ factory.get_explainer_agent(use_langchain=False)
   â†“
5. agent4_factory.py sees use_langchain=False
   â†“
6. Returns: LLMExplainerAgent (Direct HTTP)
   â†“
7. agent4_llm_explainer.py generates explanation
   - Uses: requests.post("http://localhost:11500/api/generate")
   - Speed: âš¡ 200ms
   â†“
8. Returns explanation to frontend
```

### Example 2: User uploads CV with **LangChain** mode

```
1. User uploads CV via frontend
   â†“
2. User toggles "ğŸ”— LangChain Mode" ON
   â†“
3. Frontend calls API: /match?use_langchain=true
   â†“
4. api.py swaps agent4 for this request only
   â†“
5. pipeline.py runs Agent 4:
   - Calls: factory.get_explainer_agent(use_langchain=True)
   â†“
6. agent4_factory.py sees use_langchain=True
   â†“
7. Tries to import and return: LangChainExplainerAgent
   - If import fails (langchain not installed) â†’ fallback to Direct HTTP
   - If import succeeds â†’ return LangChain agent
   â†“
8. agent4_langchain_explainer.py generates explanation
   - Uses: ChatOllama LCEL chain
   - Endpoint: still http://localhost:11500 (same Ollama)
   - Speed: ğŸ”— 300ms (slightly slower due to framework overhead)
   â†“
9. Returns explanation to frontend
```

---

## â“ FAQ: Your Questions Answered

### Q1: Are these separate scripts I need to run?
**A: NO!** They are **modules** (Python classes) that work together automatically.

- âŒ **NOT** like: `python agent4_factory.py` (don't run this)
- âœ… **YES** like: Pipeline imports and uses them automatically

### Q2: Do I need to implement them as scripts?
**A: NO!** They are already implemented as **classes/functions** inside your pipeline.

- Pipeline imports them: `from .agent4_factory import get_explainer_agent`
- API uses them: `pipeline.agent4.generate_explanation(...)`

### Q3: Which one is actually running?
**A: Depends on the toggle in the frontend!**

- **Default:** agent4_llm_explainer.py (Direct HTTP - Fast) âš¡
- **Toggle ON:** agent4_langchain_explainer.py (LangChain - Advanced) ğŸ”—

### Q4: Do I need all three files?
**A: YES, they work as a team:**

1. **agent4_factory.py** - The "decider" (chooses which agent)
2. **agent4_llm_explainer.py** - Fast mode implementation
3. **agent4_langchain_explainer.py** - Advanced mode implementation

### Q5: What happens if I delete one file?
- Delete **factory** â†’ âŒ Pipeline breaks (can't find agent4)
- Delete **llm_explainer** â†’ âŒ Direct HTTP mode breaks
- Delete **langchain_explainer** â†’ âš ï¸ LangChain mode breaks, but Direct HTTP still works (fallback)

### Q6: How do I switch modes?
**Three ways:**

**Method 1: Frontend Toggle (Per-Request)**
```typescript
// Frontend: page.tsx
const [useLangChain, setUseLangChain] = useState(false);

// Toggle visible when AI Mode is ON
{useLLM && (
  <div>ğŸ”— LangChain Mode toggle</div>
)}
```

**Method 2: Config File (Global Default)**
```python
# config.py
class LLMConfig:
    use_langchain: bool = False  # Change to True for default LangChain
```

**Method 3: API Request (Programmatic)**
```bash
curl -X POST "http://localhost:8000/match?use_langchain=true"
```

---

## ğŸ› ï¸ Current Status

### âœ… What's Working:
1. âœ… **agent4_llm_explainer.py** - Original Direct HTTP (fully working)
2. âœ… **agent4_factory.py** - Factory pattern (created, ready)
3. âœ… **agent4_langchain_explainer.py** - LangChain implementation (created, needs testing)
4. âœ… **pipeline.py** - Uses factory pattern (updated)
5. âœ… **api.py** - Per-request mode switching (updated)
6. âœ… **Frontend** - LangChain toggle UI (added)

### â³ What Needs to Be Done:
1. âŒ **Install LangChain dependencies**
   ```bash
   pip install langchain-ollama langchain-core
   ```

2. âŒ **Restart backend server**
   ```bash
   python src/api.py
   ```

3. âŒ **Test both modes**
   - Direct HTTP: Upload CV with toggle OFF
   - LangChain: Upload CV with toggle ON

---

## ğŸ§ª Testing Commands

### Test 1: Check which mode is active
```bash
# Check logs when server starts
python src/api.py

# Should see:
# âœ… Agent 4 (Explainer) ready - LangChain: False  (Direct HTTP default)
```

### Test 2: Test Direct HTTP mode
```bash
# Run pytest
pytest tests/test_agent4_modes.py::test_direct_http_mode -v -s

# Or use frontend: Toggle AI Mode ON, LangChain OFF
```

### Test 3: Test LangChain mode
```bash
# Run pytest
pytest tests/test_agent4_modes.py::test_langchain_mode -v -s

# Or use frontend: Toggle both AI Mode ON and LangChain ON
```

### Test 4: Test factory fallback
```bash
pytest tests/test_agent4_modes.py::test_factory_fallback -v -s
```

---

## ğŸ¨ Visual Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               YOUR RECRUITER-PRO-AI SYSTEM                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  ğŸ“ src/agents/                                            â”‚
â”‚    â”œâ”€â”€ agent1_parser.py         (CV text extraction)      â”‚
â”‚    â”œâ”€â”€ agent2_extractor.py      (Data structuring)        â”‚
â”‚    â”œâ”€â”€ agent3_scorer.py         (Match scoring)           â”‚
â”‚    â”‚                                                       â”‚
â”‚    â”œâ”€â”€ ğŸ­ agent4_factory.py      â† THE SWITCHER          â”‚
â”‚    â”‚      â””â”€â”€ Chooses which Agent 4 to use                â”‚
â”‚    â”‚                                                       â”‚
â”‚    â”œâ”€â”€ âš¡ agent4_llm_explainer.py                         â”‚
â”‚    â”‚      â””â”€â”€ Fast Direct HTTP (default)                  â”‚
â”‚    â”‚                                                       â”‚
â”‚    â””â”€â”€ ğŸ”— agent4_langchain_explainer.py                   â”‚
â”‚           â””â”€â”€ Advanced LangChain (optional)               â”‚
â”‚                                                            â”‚
â”‚  ğŸ“ src/                                                   â”‚
â”‚    â”œâ”€â”€ pipeline.py              (Orchestrates agents)     â”‚
â”‚    â””â”€â”€ api.py                   (FastAPI endpoints)       â”‚
â”‚                                                            â”‚
â”‚  ğŸ“ frontend/                                              â”‚
â”‚    â”œâ”€â”€ app/page.tsx             (UI with toggles)         â”‚
â”‚    â””â”€â”€ lib/api.ts               (API client)              â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Next Steps

**IMMEDIATE ACTIONS:**

1. **Install dependencies:**
   ```bash
   cd c:\Users\DELL\Desktop\Recruiter-Pro-AI
   pip install langchain-ollama langchain-core
   ```

2. **Restart server:**
   ```bash
   python src/api.py
   ```

3. **Test in browser:**
   - Open frontend
   - Upload a CV
   - Try both modes:
     - âš¡ **Direct HTTP:** Fast, proven
     - ğŸ”— **LangChain:** Advanced, new features

4. **Check logs:**
   - Look for: `âœ… Agent 4 (Explainer) ready - LangChain: False`
   - When toggling: `ğŸ”„ Switched to LangChain mode for this request`

---

## ğŸ’¡ Key Takeaway

**Think of it like a car with two engines:**

- ğŸï¸ **Direct HTTP** = Gasoline engine (fast, efficient, proven)
- ğŸ”‹ **LangChain** = Electric engine (advanced features, cleaner code)
- ğŸ­ **Factory** = Transmission (switches between engines)

**You don't run engines separately - the car (pipeline) uses them automatically!**

The three files are **not scripts**, they are **components** of your Agent 4 system that work together seamlessly.
